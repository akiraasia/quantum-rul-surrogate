{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c2zPcfo-baq2"
      },
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "!pip install qiskit qiskit-aer qiskit-machine-learning scikit-optimize lazypredict --quiet\n",
        "\n",
        "# === Imports ===\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Quantum\n",
        "from qiskit_aer import Aer\n",
        "from qiskit.utils import algorithm_globals, QuantumInstance\n",
        "from qiskit.circuit.library import ZZFeatureMap, TwoLocal\n",
        "from qiskit_machine_learning.algorithms import VQC\n",
        "\n",
        "# Bayesian Optimization\n",
        "from skopt import gp_minimize\n",
        "from skopt.space import Integer, Real\n",
        "from skopt.utils import use_named_args\n",
        "\n",
        "# LazyPredict (optional)\n",
        "from lazypredict.Supervised import LazyRegressor\n",
        "\n",
        "# === Load and preprocess dataset ===\n",
        "df_raw = pd.read_csv(\"train_FD001.txt\", sep='\\s+', header=None)\n",
        "df_raw.columns = ['engine_id', 'time_in_cycles', 'op_setting_1', 'op_setting_2', 'op_setting_3'] + \\\n",
        "                 [f'sensor_measurement_{i}' for i in range(1, 22)]\n",
        "df_raw['RUL'] = df_raw.groupby('engine_id')['time_in_cycles'].transform(max) - df_raw['time_in_cycles']\n",
        "\n",
        "features = df_raw.drop(columns=['engine_id', 'time_in_cycles', 'RUL'])\n",
        "scaler = StandardScaler()\n",
        "features_scaled = scaler.fit_transform(features)\n",
        "df_scaled = pd.DataFrame(features_scaled, columns=features.columns)\n",
        "df_scaled['engine_id'] = df_raw['engine_id']\n",
        "df_scaled['time_in_cycles'] = df_raw['time_in_cycles']\n",
        "df_scaled['RUL'] = df_raw['RUL']\n",
        "df_scaled.to_csv(\"train_FD001_normalized.csv\", index=False)\n",
        "\n",
        "# === Interval training ===\n",
        "def train_interval_models(df, num_intervals):\n",
        "    interval_edges = np.linspace(df['RUL'].min(), df['RUL'].max(), num_intervals + 1)\n",
        "    df['interval_id'] = pd.cut(df['RUL'], bins=interval_edges, labels=False, include_lowest=True)\n",
        "    interval_groups = [group for _, group in df.groupby('interval_id')]\n",
        "    trained_models = []\n",
        "    for group in interval_groups:\n",
        "        if len(group) < 10:\n",
        "            trained_models.append((None, None))\n",
        "            continue\n",
        "        X = group.drop(columns=['engine_id', 'time_in_cycles', 'RUL', 'interval_id'])\n",
        "        y = group['RUL']\n",
        "        try:\n",
        "            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "            model = make_pipeline(StandardScaler(), Ridge())\n",
        "            model.fit(X_train, y_train)\n",
        "            trained_models.append(('Ridge', model))\n",
        "        except:\n",
        "            trained_models.append((None, None))\n",
        "    return interval_groups, trained_models\n",
        "\n",
        "# === Loss calculation ===\n",
        "def compute_total_loss(df, num_intervals, lambda_weight, penalty_weight, cache):\n",
        "    if num_intervals not in cache:\n",
        "        groups, models = train_interval_models(df.copy(), num_intervals)\n",
        "        cache[num_intervals] = (groups, models)\n",
        "    else:\n",
        "        groups, models = cache[num_intervals]\n",
        "    i1, i2 = 1, 2\n",
        "    if i1 >= len(groups) or i2 >= len(groups):\n",
        "        return np.inf\n",
        "    model_1, model_2 = models[i1][1], models[i2][1]\n",
        "    if model_1 is None or model_2 is None:\n",
        "        return np.inf\n",
        "    group_1, group_2 = groups[i1], groups[i2]\n",
        "    merged_group = pd.concat([group_1, group_2])\n",
        "    X_merged = merged_group.drop(columns=['engine_id', 'time_in_cycles', 'RUL', 'interval_id'])\n",
        "    y_merged = merged_group['RUL']\n",
        "    merged_model = make_pipeline(StandardScaler(), Ridge())\n",
        "    merged_model.fit(X_merged, y_merged)\n",
        "    Aa_pred = merged_model.predict(X_merged)\n",
        "    X1 = group_1.drop(columns=['engine_id', 'time_in_cycles', 'RUL', 'interval_id'])\n",
        "    X2 = group_2.drop(columns=['engine_id', 'time_in_cycles', 'RUL', 'interval_id'])\n",
        "    a1_pred = model_1.predict(X1)\n",
        "    a2_pred = model_2.predict(X2)\n",
        "    a_a = np.concatenate([lambda_weight * a1_pred, (1 - lambda_weight) * a2_pred])\n",
        "    A_a = Aa_pred\n",
        "    model_diff = np.sqrt(mean_squared_error(A_a, a_a))\n",
        "    penalty = penalty_weight * np.abs(np.mean(A_a - a_a))\n",
        "    return model_diff + penalty\n",
        "\n",
        "# === Bayesian Optimization ===\n",
        "search_space = [\n",
        "    Integer(3, 6, name='num_intervals'),\n",
        "    Real(0.1, 0.9, name='lambda_weight'),\n",
        "    Real(0.001, 0.1, prior='log-uniform', name='penalty_weight')\n",
        "]\n",
        "\n",
        "cache = {}\n",
        "\n",
        "@use_named_args(search_space)\n",
        "def objective(num_intervals, lambda_weight, penalty_weight):\n",
        "    loss = compute_total_loss(df_scaled, num_intervals, lambda_weight, penalty_weight, cache)\n",
        "    print(f\"Intervals: {num_intervals}, Lambda: {lambda_weight:.3f}, Penalty: {penalty_weight:.5f} => Loss: {loss:.4f}\")\n",
        "    return loss\n",
        "\n",
        "print(\"Starting Bayesian Optimization...\")\n",
        "result = gp_minimize(objective, dimensions=search_space, n_calls=30, n_random_starts=10, random_state=42)\n",
        "\n",
        "# === Quantum model training ===\n",
        "def train_quantum_surrogate(X_train, y_train, X_test, y_test):\n",
        "    n_classes = 5\n",
        "    y_train_classes = pd.qcut(y_train, n_classes, labels=False)\n",
        "    y_test_classes = pd.qcut(y_test, n_classes, labels=False)\n",
        "    feature_map = ZZFeatureMap(feature_dimension=X_train.shape[1], reps=2)\n",
        "    ansatz = TwoLocal(num_qubits=X_train.shape[1], reps=1, rotation_blocks=['ry', 'rz'], entanglement='cz')\n",
        "    algorithm_globals.random_seed = 42\n",
        "    quantum_instance = QuantumInstance(backend=Aer.get_backend('aer_simulator_statevector'), seed_simulator=42, seed_transpiler=42)\n",
        "    vqc = VQC(feature_map=feature_map, ansatz=ansatz, quantum_instance=quantum_instance)\n",
        "    vqc.fit(X_train, y_train_classes)\n",
        "    y_pred_classes = vqc.predict(X_test)\n",
        "    bins = pd.qcut(y_train, n_classes, retbins=True)[1]\n",
        "    bin_centers = (bins[:-1] + bins[1:]) / 2\n",
        "    y_pred_rul = np.array([bin_centers[int(c)] for c in y_pred_classes])\n",
        "    rmse = np.sqrt(mean_squared_error(y_test, y_pred_rul))\n",
        "    return rmse\n",
        "\n",
        "# === Compare with quantum surrogate ===\n",
        "interval_groups, trained_models = train_interval_models(df_scaled, result.x[0])\n",
        "group = next((g for g in interval_groups if len(g) >= 50), None)\n",
        "if group is not None:\n",
        "    X = group.drop(columns=['engine_id', 'time_in_cycles', 'RUL', 'interval_id'])\n",
        "    y = group['RUL']\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "    print(\"\\nTraining quantum surrogate model...\")\n",
        "    quantum_rmse = train_quantum_surrogate(X_train.values, y_train.values, X_test.values, y_test.values)\n",
        "    print(f\"Quantum surrogate RMSE: {quantum_rmse:.4f}\")\n",
        "else:\n",
        "    print(\"No interval group large enough for quantum surrogate training demo.\")\n",
        "\n",
        "# === Save Results ===\n",
        "print(\"\\nBest parameters:\")\n",
        "print(f\"Intervals: {result.x[0]}\")\n",
        "print(f\"Lambda: {result.x[1]:.4f}\")\n",
        "print(f\"Penalty: {result.x[2]:.6f}\")\n",
        "print(f\"Lowest Loss: {result.fun:.4f}\")"
      ]
    }
  ]
}